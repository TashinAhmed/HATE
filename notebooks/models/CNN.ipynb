{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c26a1-e23f-47f4-89fc-a9e2b19e1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package installation\n",
    "!pip install pyicu\n",
    "!pip install pycld2\n",
    "!pip install polyglot\n",
    "!pip install textstat\n",
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14eaafd-3d39-4468-814b-4881d8d5ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import folium\n",
    "import textstat\n",
    "from scipy import stats\n",
    "from colorama import Fore, Back, Style, init\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import networkx as nx\n",
    "from pandas import Timestamp\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "\n",
    "import requests\n",
    "from IPython.display import HTML\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import accuracy_score, rocaucscore\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Embedding\n",
    "from tensorflow.keras.layers import LSTM, GRU, Conv1D, SpatialDropout1D\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras.constraints import *\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,\\\n",
    "                                            CountVectorizer,\\\n",
    "                                            HashingVectorizer\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from googletrans import Translator\n",
    "from nltk import WordNetLemmatizer\n",
    "from polyglot.detect import Detector\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "stopword=set(STOPWORDS)\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd658e-c266-4b34-bb30-c5245fd007f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/\"\n",
    "os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdc88f-8fa3-4b04-8446-5f9d273356b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path configs\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "VAL_PATH = DATA_PATH + \"validation.csv\"\n",
    "TRAIN_PATH = DATA_PATH + \"jigsaw-toxic-comment-train.csv\"\n",
    "\n",
    "val_data = pd.read_csv(VAL_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "train_data = pd.read_csv(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3dfb7-d818-4c6e-93ba-c6ee62df88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val_data\n",
    "train = train_data\n",
    "\n",
    "def clean(text):\n",
    "    text = text.fillna(\"fillna\").str.lower()\n",
    "    text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\(http://.*?\\s\\(http://.*\\)\",'',str(x)))\n",
    "    return text\n",
    "\n",
    "val[\"comment_text\"] = clean(val[\"comment_text\"])\n",
    "test_data[\"content\"] = clean(test_data[\"content\"])\n",
    "train[\"comment_text\"] = clean(train[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c9216-1e08-4593-accf-9ac875775212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = rocaucscore(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc7c54-bf5e-461b-91bb-9f3c4225c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=240, maxlen=512):\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d82e91-9918-4121-9375-27148e333aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path('jigsaw-multilingual-toxic-comment-classification')\n",
    "\n",
    "#EPOCHS = 2\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96f0d8-0c02-454d-8cc0-7b20ef54a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "save_path = '/kaggle/working/distilbert_base_uncased/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "fast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', \n",
    "                                        lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48b251-cc01-4fbe-9cb3-bd406ec9d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fast_encode(train.comment_text.astype(str), fast_tokenizer, maxlen=512)\n",
    "x_valid = fast_encode(val_data.comment_text.astype(str).values, fast_tokenizer, maxlen=512)\n",
    "x_test = fast_encode(test_data.content.astype(str).values, fast_tokenizer, maxlen=512)\n",
    "\n",
    "y_valid = val.toxic.values\n",
    "y_train = train.toxic.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681419ef-0c71-4e08-85fc-dc85f0bb41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b6977-67c7-4b9e-b299-9be38f5f2a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custome_cnn_model(transformer, max_len):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    \n",
    "    embed = transformer.weights[0].numpy()\n",
    "    embedding = Embedding(np.shape(embed)[0], np.shape(embed)[1],\n",
    "                          input_length=max_len, weights=[embed],\n",
    "                          trainable=False)(input_word_ids)\n",
    "    \n",
    "    embedding = SpatialDropout1D(0.3)(embedding)\n",
    "    c1 = Conv1D(64, 2)(embedding)\n",
    "    c2 = Conv1D(64, 3)(embedding)\n",
    "    c3 = Conv1D(64, 4)(embedding)\n",
    "    c4 = Conv1D(64, 5)(embedding)\n",
    "    \n",
    "    maxpool_1 = GlobalAveragePooling1D()(c1)\n",
    "    maxpool_2 = GlobalAveragePooling1D()(c2)\n",
    "    maxpool_3 = GlobalAveragePooling1D()(c3)\n",
    "    maxpool_4 = GlobalAveragePooling1D()(c4)\n",
    "    ccat = ccatatenate([maxpool_1, maxpool_2, maxpool_3, maxpool_4], axis=1)\n",
    "\n",
    "    ccat = Dense(64, activation='relu')(ccat)\n",
    "    ccat = Dense(1, activation='sigmoid')(ccat)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=ccat)\n",
    "    \n",
    "    model.compile(Adam(lr=0.01), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3504d9a-f693-4127-8de6-fbba59f755f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = custome_cnn_model(transformer_layer, max_len=512)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1ac20-e3dc-443b-a2ed-3eac530dce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84942a3c-2fb1-454c-bd67-6e3bbcc03e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback():\n",
    "    cb = []\n",
    "\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss',  \n",
    "                                    factor=0.3, patience=3, \n",
    "                                    verbose=1, mode='auto', \n",
    "                                    epsilon=0.0001, cooldown=1, min_lr=0.000001)\n",
    "    cb.append(reduceLROnPlat)\n",
    "    log = CSVLogger('log.csv')\n",
    "    cb.append(log)\n",
    "\n",
    "    RocAuc = RocAucEvaluation(validation_data=(x_valid, y_valid), interval=1)\n",
    "    cb.append(RocAuc)\n",
    "    \n",
    "    return cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb390c17-5f8a-4411-8b69-9f4c6b34ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = x_train.shape[0] // BATCH_SIZE\n",
    "calls = callback()\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=N_STEPS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks = calls,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927330bc-9c69-4eae-a29c-ed0512b31b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "\n",
    "def visualize_model_preds(model, indices=[0, 17, 1, 24]):\n",
    "    comments = val_data.comment_text.loc[indices].values.tolist()\n",
    "    preds = model.predict(x_valid[indices].reshape(len(indices), -1))\n",
    "\n",
    "    for idx, i in enumerate(indices):\n",
    "        if y_valid[i] == 0:\n",
    "            label = \"Non-toxic\"\n",
    "            color = f'{Fore.GREEN}'\n",
    "            symbol = '\\u2714'\n",
    "        else:\n",
    "            label = \"Toxic\"\n",
    "            color = f'{Fore.RED}'\n",
    "            symbol = '\\u2716'\n",
    "\n",
    "        print('{}{} {}'.format(color, str(idx+1) + \". \" + label, symbol))\n",
    "        print(f'{Style.RESET_ALL}')\n",
    "        print(\"ORIGINAL\")\n",
    "        print(comments[idx]); print(\"\")\n",
    "        print(\"TRANSLATED\")\n",
    "        print(translator.translate(comments[idx]).text)\n",
    "        fig = go.Figure()\n",
    "        if list.index(sorted(preds[:, 0]), preds[idx][0]) > 1:\n",
    "            yl = [preds[idx][0], 1 - preds[idx][0]]\n",
    "        else:\n",
    "            yl = [1 - preds[idx][0], preds[idx][0]]\n",
    "        fig.add_trace(go.Bar(x=['Non-Toxic', 'Toxic'], y=yl, marker=dict(color=[\"seagreen\", \"indianred\"])))\n",
    "        fig.update_traces(name=comments[idx])\n",
    "        fig.update_layout(xaxis_title=\"Labels\", yaxis_title=\"Probability\", template=\"plotly_white\", title_text=\"Predictions for validation comment #{}\".format(idx+1))\n",
    "        fig.show()\n",
    "        \n",
    "visualize_model_preds(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9382a11-a756-4cfa-9a3c-3b0ccf6158c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbbc4a-81a7-4450-a808-a10419eb8f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8250c-49f6-4544-a843-ffb36c4ed74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
